
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../01_multiple_ai_apis/">
      
      
        <link rel="next" href="../../03_open-source-solutions/overview/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Streaming AI Responses - My LLM Journey</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#streaming-ai-responses" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="My LLM Journey" class="md-header__button md-logo" aria-label="My LLM Journey" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My LLM Journey
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Streaming AI Responses
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="My LLM Journey" class="md-nav__button md-logo" aria-label="My LLM Journey" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My LLM Journey
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Build Your First LLM Product
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Build Your First LLM Product
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_first_llm_product/01_jumping_right_llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jumping Right into LLM Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_first_llm_product/02_setting-up-ollama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Setting Up Ollama for Local LLM Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_first_llm_product/03_llm-engineering-roadmap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Engineering Roadmap
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Build a Multi-Modal Chatbot
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Build a Multi-Modal Chatbot
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_multiple_ai_apis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multiple AI APIs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Streaming AI Responses
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Streaming AI Responses
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#environment-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Environment Configuration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-message-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Common Message Structure
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#connecting-to-apis" class="md-nav__link">
    <span class="md-ellipsis">
      Connecting to APIs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Connecting to APIs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-claude" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic (Claude)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-gemini" class="md-nav__link">
    <span class="md-ellipsis">
      Google (Gemini)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama (Local Models)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-joke-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Joke Generation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Streaming Responses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Streaming Responses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-gemini_1" class="md-nav__link">
    <span class="md-ellipsis">
      Google Gemini
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Streaming
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#handling-markdown-streaming-in-jupyter" class="md-nav__link">
    <span class="md-ellipsis">
      Handling Markdown Streaming in Jupyter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparison-of-apis" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of APIs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      Next Steps
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Open-Source Gen AI Solutions
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Open-Source Gen AI Solutions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../03_open-source-solutions/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Evaluation for Code & Business Tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Model Evaluation for Code & Business Tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../04_model-evaluation/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    RAG with LangChain
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            RAG with LangChain
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../05_rag-with-langchain/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fine-Tuning with LoRA/QLoRA
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Fine-Tuning with LoRA/QLoRA
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../06_fine-tuning/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fine-Tuned Price Prediction Model
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Fine-Tuned Price Prediction Model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../07_fine-tuned-price-prediction/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Autonomous Multi-Agent Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Autonomous Multi-Agent Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../08_autonomous-multi-agent/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#environment-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Environment Configuration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-message-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Common Message Structure
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#connecting-to-apis" class="md-nav__link">
    <span class="md-ellipsis">
      Connecting to APIs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Connecting to APIs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-claude" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic (Claude)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-gemini" class="md-nav__link">
    <span class="md-ellipsis">
      Google (Gemini)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-local-models" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama (Local Models)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-joke-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Joke Generation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Streaming Responses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Streaming Responses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-gemini_1" class="md-nav__link">
    <span class="md-ellipsis">
      Google Gemini
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Streaming
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#handling-markdown-streaming-in-jupyter" class="md-nav__link">
    <span class="md-ellipsis">
      Handling Markdown Streaming in Jupyter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparison-of-apis" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of APIs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      Next Steps
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="streaming-ai-responses">Streaming AI Responses</h1>
<p>This document describes how to implement real-time LLM output (“streaming”) in Python using four providers: OpenAI, Anthropic (Claude), Google (Gemini), and local models via Ollama. It covers environment setup, key management (where required), code examples for both synchronous and streaming interactions, API parameter explanations, and best practices.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Python 3.8+</li>
<li>
<p>For OpenAI, Anthropic, Google:</p>
</li>
<li>
<p>Accounts and API keys</p>
</li>
<li>
<p>Installed SDKs</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>openai<span class="w"> </span>anthropic<span class="w"> </span>google-generativeai
</code></pre></div>
</li>
<li>
<p>For Ollama (local models):</p>
</li>
<li>
<p>Docker (macOS or Linux) or Windows Subsystem for Linux (WSL)</p>
</li>
<li>Ollama daemon installed and running</li>
<li>
<p>Ollama Python client library</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>ollama
</code></pre></div>
</li>
</ul>
<h2 id="environment-configuration">Environment Configuration</h2>
<ol>
<li>Create a file named <code>.env</code> in your project root.</li>
<li>Populate it with provider keys (skip for Ollama):</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>your_openai_key
<span class="nv">ANTHROPIC_API_KEY</span><span class="o">=</span>your_anthropic_key
<span class="nv">GOOGLE_API_KEY</span><span class="o">=</span>your_google_key
</code></pre></div>
<ol>
<li>Load environment variables in code rather than embedding keys directly:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span>   <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">]</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;GOOGLE_API_KEY&quot;</span><span class="p">]</span>   <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="common-message-structure">Common Message Structure</h2>
<p>All four interfaces use a chat-style message list or a simple prompt. Each message is a dict with:</p>
<ul>
<li><code>role</code>: one of <code>"system"</code>, <code>"user"</code> (and <code>"assistant"</code> in responses).</li>
<li><code>content</code>: the text to send.</li>
</ul>
<p>Additional parameters may include:</p>
<ul>
<li><code>model</code>: model identifier (e.g., <code>gpt-4</code>, <code>claude-3.5-sonnet</code>, <code>gemini-1.5-flash</code>, <code>llama3</code>)</li>
<li><code>temperature</code>: float in [0,1] (higher = more creative)</li>
<li><code>max_tokens</code> / <code>n_predict</code> / <code>num_predict</code>: cap on output length</li>
<li><code>stream</code>: boolean for OpenAI; method switch or flag for others</li>
</ul>
<h2 id="connecting-to-apis">Connecting to APIs</h2>
<h3 id="openai">OpenAI</h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span><span class="o">,</span><span class="w"> </span><span class="nn">os</span>

<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;You are an assistant great at telling jokes.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Tell a lighthearted joke for data scientists.&quot;</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>
<h3 id="anthropic-claude">Anthropic (Claude)</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">Anthropic</span><span class="p">,</span> <span class="n">HUMAN_PROMPT</span><span class="p">,</span> <span class="n">AI_PROMPT</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Anthropic</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">))</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3.5-sonnet&quot;</span><span class="p">,</span>
    <span class="n">max_tokens_to_sample</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="p">(</span><span class="n">HUMAN_PROMPT</span> <span class="o">+</span>
            <span class="s2">&quot;Tell a lighthearted joke for data scientists.</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>
            <span class="n">AI_PROMPT</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">completion</span><span class="p">)</span>
</code></pre></div>
<h3 id="google-gemini">Google (Gemini)</h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">google.generativeai</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">genai</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">genai</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY&quot;</span><span class="p">))</span>
<span class="n">gemini</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">TextGenerationModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gemini-1.5-flash&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">gemini</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Tell a lighthearted joke for data scientists.&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</code></pre></div>
<h3 id="ollama-local-models">Ollama (Local Models)</h3>
<ol>
<li>Pull or install a model, e.g.:</li>
</ol>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>pull<span class="w"> </span>llama3
</code></pre></div>
<ol>
<li>Verify available models:</li>
</ol>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>list
</code></pre></div>
<ol>
<li>Functional interface:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">chat</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Tell a lighthearted joke for data scientists.&quot;</span><span class="p">}],</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.7</span><span class="p">,</span><span class="s2">&quot;num_predict&quot;</span><span class="p">:</span><span class="mi">256</span><span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>
</code></pre></div>
<ol>
<li>Client interface:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>  <span class="c1"># connects to localhost:11434</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Tell a lighthearted joke for data scientists.&quot;</span><span class="p">}],</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.7</span><span class="p">,</span><span class="s2">&quot;num_predict&quot;</span><span class="p">:</span><span class="mi">256</span><span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>
<h2 id="example-joke-generation">Example: Joke Generation</h2>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Prompt</th>
<th>Sample Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>“Tell a lighthearted joke for data scientists.”</td>
<td>“Why did the data scientists break up with their computer? It couldn’t handle their complex relationship.”</td>
</tr>
<tr>
<td>GPT-4</td>
<td>same</td>
<td>“Why did the data scientist break up with a statistician? Because she found him too mean.”</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>same</td>
<td>“Why do data scientists break up with their significant other? Too much variance in the relationship.”</td>
</tr>
<tr>
<td>Gemini 1.5 Flash</td>
<td>same</td>
<td>“Why did the data scientists break up with a statistician? Because they couldn’t see eye to eye on the p value.”</td>
</tr>
<tr>
<td>Llama3 (Ollama)</td>
<td>same</td>
<td>“Why did the data scientist break up with their database? It lost all their relationships.”</td>
</tr>
</tbody>
</table>
<p>Adjust <code>temperature</code> or <code>num_predict</code> for variation in randomness and length.</p>
<h2 id="streaming-responses">Streaming Responses</h2>
<h3 id="openai-streaming">OpenAI Streaming</h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatCompletion</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">))</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
</code></pre></div>
<h3 id="anthropic-streaming">Anthropic Streaming</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">Anthropic</span><span class="p">,</span> <span class="n">HUMAN_PROMPT</span><span class="p">,</span> <span class="n">AI_PROMPT</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Anthropic</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">))</span>
<span class="k">with</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3.5-sonnet&quot;</span><span class="p">,</span>
    <span class="n">max_tokens_to_sample</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">HUMAN_PROMPT</span> <span class="o">+</span> <span class="n">user_prompt</span> <span class="o">+</span> <span class="n">AI_PROMPT</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="o">.</span><span class="n">completion</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h3 id="google-gemini_1">Google Gemini</h3>
<p>Streaming not supported; use synchronous only.</p>
<h3 id="ollama-streaming">Ollama Streaming</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">chat</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Explain how to decide if a business problem is suitable for an LLM solution.&quot;</span><span class="p">}],</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.7</span><span class="p">,</span><span class="s2">&quot;num_predict&quot;</span><span class="p">:</span><span class="mi">512</span><span class="p">}</span>
<span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>Client-based equivalent:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Explain how to decide if a business problem is suitable for an LLM solution.&quot;</span><span class="p">}],</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.7</span><span class="p">,</span><span class="s2">&quot;num_predict&quot;</span><span class="p">:</span><span class="mi">512</span><span class="p">}</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h2 id="handling-markdown-streaming-in-jupyter">Handling Markdown Streaming in Jupyter</h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">clear_output</span><span class="p">,</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>

<span class="n">buffer</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="n">buffer</span> <span class="o">+=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">buffer</span><span class="p">))</span>
</code></pre></div>
<p>Use the same pattern for Anthropic and Ollama streams.</p>
<h2 id="comparison-of-apis">Comparison of APIs</h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>OpenAI</th>
<th>Anthropic (Claude)</th>
<th>Google (Gemini)</th>
<th>Ollama (Local)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Endpoint style</td>
<td><code>ChatCompletion.create</code></td>
<td><code>chat.completions.create</code></td>
<td><code>model.generate</code></td>
<td><code>chat(...)</code> / <code>Client.chat</code></td>
</tr>
<tr>
<td>Streaming activation</td>
<td><code>stream=True</code></td>
<td><code>.stream(...)</code> method</td>
<td>Not supported</td>
<td><code>stream=True</code> flag</td>
</tr>
<tr>
<td>System message placement</td>
<td>In messages list</td>
<td>Separate <code>prompt</code> prefix</td>
<td>In constructor</td>
<td>In messages list</td>
</tr>
<tr>
<td>Token cap param</td>
<td><code>max_tokens</code></td>
<td><code>max_tokens_to_sample</code></td>
<td>Implicit</td>
<td><code>num_predict</code></td>
</tr>
<tr>
<td>Context window</td>
<td>8K–128K tokens</td>
<td>\~100K tokens</td>
<td>1 000 000 tokens</td>
<td>Model-dependent</td>
</tr>
</tbody>
</table>
<h2 id="best-practices">Best Practices</h2>
<ul>
<li><strong>Key Management</strong>: Store in <code>.env</code>, never hard-code (not required for Ollama).</li>
<li><strong>Error Handling</strong>: Catch provider-specific exceptions.</li>
<li><strong>Rate Limits</strong>: Respect external API limits; local models unrestricted.</li>
<li><strong>Chunk Processing</strong>: Accumulate partial content; avoid per-chunk newlines.</li>
<li><strong>Token Budgeting</strong>: Use <code>max_tokens</code>/<code>num_predict</code> to control length.</li>
<li><strong>Reproducibility</strong>: Set <code>temperature=0</code> for deterministic output.</li>
<li><strong>Local vs Cloud</strong>: Use Ollama for offline or privacy-sensitive tasks.</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<ul>
<li>Enable model-to-model “self-play” conversations.</li>
<li>Integrate multi-turn context management.</li>
<li>Explore fine-tuning or embeddings pipelines.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../../javascripts/mermaid.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
    
  </body>
</html>