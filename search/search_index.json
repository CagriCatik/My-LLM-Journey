{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My LLM Journey","text":""},{"location":"#1-build-your-first-llm-product","title":"1: Build Your First LLM Product","text":"<p>This first module guides you through every step needed to launch a working LLM application from scratch. You will install and configure local inference tools, deploy models with Ollama on Windows and Mac, and build a practical Spanish\u2010tutoring assistant to see results immediately. Along the way, you\u2019ll establish a robust development environment-covering Conda, virtualenv, API key management, and best practices for Jupyter Lab-so you can iterate rapidly. By comparing leading models (OpenAI, Ollama, Claude, Gemini) and completing hands-on projects like a web-page summarizer, you\u2019ll acquire the core skills and insights required to progress from beginner to confident LLM engineer.</p> Click to expand Module Overview <ul> <li> Jumping Right into LLM Engineering  </li> <li> Setting Up Ollama for Local LLM Deployment</li> <li> LLM Engineering Roadmap</li> <li>Building LLM Applications: Chatbots, RAG, and Agentic AI Projects  </li> <li>From Wall Street to AI: Ed Donner's Path to Becoming an LLM Engineer  </li> <li>Setting Up Your LLM Development Environment: Tools and Best Practices  </li> <li>Mac Setup Guide: Jupyter Lab and Conda for LLM Projects  </li> <li>Setting Up Anaconda for LLM Engineering: Windows Installation Guide  </li> <li>Alternative Python Setup for LLM Projects: Virtualenv vs. Anaconda Guide  </li> <li>Setting Up OpenAI API for LLM Development: Keys, Pricing &amp; Best Practices  </li> <li>Creating a .env File for Storing API Keys Safely  </li> <li>Instant Gratification Project: Creating an AI-Powered Web Page Summarizer  </li> <li>Implementing Text Summarization Using OpenAI's GPT-4 and Beautiful Soup  </li> <li>Wrapping Up: Key Takeaways and Next Steps in LLM Engineering  </li> <li>LLM Engineering: Key Skills and Tools for AI Development  </li> <li>Understanding Frontier Models: GPT, Claude, and Open Source LLMs  </li> <li>How to Use Ollama for Local LLM Inference: Python Tutorial with Jupyter  </li> <li>Hands-On LLM Task: Comparing OpenAI and Ollama for Text Summarization  </li> <li>Frontier AI Models: Comparing GPT-4, Claude, Gemini, and LLAMA  </li> <li>Comparing Leading LLMs: Strengths and Business Applications  </li> <li>Exploring GPT-4o vs O1 Preview: Key Differences in Performance  </li> <li>Creativity and Coding: Leveraging GPT-4o\u2019s Canvas Feature  </li> <li>Claude 3.5\u2019s Alignment and Artifact Creation: A Deep Dive  </li> <li>AI Model Comparison: Gemini vs Cohere for Whimsical and Analytical Tasks  </li> <li>Evaluating Meta AI and Perplexity: Nuances of Model Outputs  </li> <li>LLM Leadership Challenge: Evaluating AI Models Through Creative Prompts  </li> <li>Revealing the Leadership Winner: A Fun LLM Challenge  </li> <li>Exploring the Journey of AI: From Early Models to Transformers  </li> <li>Understanding LLM Parameters: From GPT-1 to Trillion-Weight Models  </li> <li>GPT Tokenization Explained: How Large Language Models Process Text Input  </li> <li>How Context Windows Impact AI Language Models: Token Limits Explained  </li> <li>Navigating AI Model Costs: API Pricing vs. Chat Interface Subscriptions  </li> <li>Comparing LLM Context Windows: GPT-4 vs Claude vs Gemini 1.5 Flash  </li> <li>Wrapping Up: Key Takeaways and Practical Insights  </li> <li>Building AI-Powered Marketing Brochures with OpenAI API and Python  </li> <li>JupyterLab Tutorial: Web Scraping for AI-Powered Company Brochures  </li> <li>Structured Outputs in LLMs: Optimizing JSON Responses for AI Projects  </li> <li>Creating and Formatting Responses for Brochure Content  </li> <li>Final Adjustments: Optimizing Markdown and Streaming in JupyterLab  </li> <li>Multi-Shot Prompting: Enhancing LLM Reliability in AI Projects  </li> <li>Assignment: Developing Your Customized LLM-Based Tutor  </li> <li>Wrapping Up: Achievements and Next Steps</li> </ul>"},{"location":"#2-build-a-multi-modal-chatbot","title":"2: Build a Multi-Modal Chatbot","text":"<p>This second module teaches you how to extend LLMs beyond text by building rich, interactive chatbots. You will integrate multiple AI APIs (OpenAI, Claude, Gemini), implement real-time streaming outputs in Python, and craft adversarial conversation flows. You\u2019ll prototype and deploy customizable UIs with Gradio-adding multi-shot prompting, context enrichment, and live function calling-to empower LLMs with external tools and code execution. Finally, you\u2019ll combine text, images, and audio into a unified multimodal assistant, integrating DALL\u00b7E image generation, sound processing, and agentic workflows for a fully featured conversational AI.</p> Click to expand Module Overview <ul> <li>Multiple AI APIs: OpenAI, Claude, and Gemini for LLM Engineers  </li> <li>Streaming AI Responses: Implementing Real-Time LLM Output in Python  </li> <li>How to Create Adversarial AI Conversations Using OpenAI and Claude APIs  </li> <li>AI Tools: Exploring Transformers &amp; Frontier LLMs for Developers  </li> <li>Building AI UIs with Gradio: Quick Prototyping for LLM Engineers  </li> <li>Gradio Tutorial: Create Interactive AI Interfaces for OpenAI GPT Models  </li> <li>Implementing Streaming Responses with GPT and Claude in Gradio UI  </li> <li>Building a Multi-Model AI Chat Interface with Gradio: GPT vs Claude  </li> <li>Building Advanced AI UIs: From OpenAI API to Chat Interfaces with Gradio  </li> <li>Building AI Chatbots: Gradio for Customer Support Assistants  </li> <li>Build a Conversational AI Chatbot with OpenAI &amp; Gradio: Step-by-Step  </li> <li>Enhancing Chatbots with Multi-Shot Prompting and Context Enrichment  </li> <li>AI Tools: Empowering LLMs to Run Code on Your Machine  </li> <li>Using AI Tools with LLMs: Enhancing Large Language Model Capabilities  </li> <li>Building an AI Airline Assistant: Implementing Tools with OpenAI GPT-4  </li> <li>How to Equip LLMs with Custom Tools: OpenAI Function Calling Tutorial  </li> <li>AI Tools: Building Advanced LLM-Powered Assistants with APIs  </li> <li>Multimodal AI Assistants: Integrating Image and Sound Generation  </li> <li>Multimodal AI: Integrating DALL-E 3 Image Generation in JupyterLab  </li> <li>Build a Multimodal AI Agent: Integrating Audio &amp; Image Tools  </li> <li>How to Build a Multimodal AI Assistant: Integrating Tools and Agents</li> </ul>"},{"location":"#3-open-source-gen-ai-solutions","title":"3: Open-Source Gen AI Solutions","text":"<p>This module dives into open-source generative AI by guiding you through the Hugging Face ecosystem and Google Colab workflows. You\u2019ll learn to browse and leverage community models, datasets, and Spaces; configure Colab notebooks with secure API key management; and run inference using Transformers pipelines for tasks like text generation, summarization, and joke creation. You\u2019ll explore tokenizer architectures-LLAMA, Phi-2, Qwen, Starcoder-and compare their performance to prepare for advanced text processing. Finally, you\u2019ll combine frontier and open-source models to build custom applications, such as an AI-powered meeting-minutes generator and a synthetic test-data generator for business use.</p> Click to expand Module Overview <ul> <li>Hugging Face Tutorial: Exploring Open-Source AI Models and Datasets  </li> <li>Exploring HuggingFace Hub: Models, Datasets &amp; Spaces for AI Developers  </li> <li>Intro to Google Colab: Cloud Jupyter Notebooks for Machine Learning  </li> <li>Hugging Face Integration with Google Colab: Secrets and API Keys Setup  </li> <li>Google Colab: Run Open-Source AI Models with Hugging Face  </li> <li>Hugging Face Transformers: Using Pipelines for AI Tasks in Python  </li> <li>Hugging Face Pipelines: Simplifying AI Tasks with Transformers Library  </li> <li>HuggingFace Pipelines: Efficient AI Inference for ML Tasks  </li> <li>Exploring Tokenizers in Open-Source AI: Llama, Phi-2, Qwen, &amp; Starcoder  </li> <li>Tokenization Techniques in AI: Using AutoTokenizer with LLAMA 3.1 Model  </li> <li>Comparing Tokenizers: Llama, PHI-3, and OWEN2 for Open-Source AI Models  </li> <li>Hugging Face Tokenizers: Preparing for Advanced AI Text Generation  </li> <li>Hugging Face Model Class: Running Inference on Open-Source AI Models  </li> <li>Hugging Face Transformers: Loading &amp; Quantizing LLMs with Bits &amp; Bytes  </li> <li>Hugging Face Transformers: Generating Jokes with Open-Source AI Models  </li> <li>Hugging Face Transformers: Models, Pipelines, and Tokenizers  </li> <li>Combining Frontier &amp; Open-Source Models for Audio-to-Text Summarization  </li> <li>Using Hugging Face &amp; OpenAI for AI-Powered Meeting Minutes Generation  </li> <li>Build a Synthetic Test Data Generator: Open-Source AI Model for Business</li> </ul>"},{"location":"#4-model-evaluation-for-code-business-tasks","title":"4: Model Evaluation for Code &amp; Business Tasks","text":"<p>This fourth module teaches you how to rigorously evaluate and compare language models for software development and enterprise use cases. You will learn criteria for selecting the right LLM-open or closed source-by examining scaling laws, benchmark limitations, and specialized leaderboards. Hands-on comparisons (GPT-4 vs. Claude vs. open-source models) will cover code generation performance, error analysis, and business-centric metrics. You will also build evaluation tools-such as Gradio UIs and Hugging Face endpoints-for systematic testing, discover common pitfalls, and master techniques to optimize model choice and integration for real-world code and business tasks.</p> Click to expand Module Overview <ul> <li>How to Choose the Right LLM: Comparing Open and Closed Source Models  </li> <li>Chinchilla Scaling Law: Optimizing LLM Parameters and Training Data Size  </li> <li>Limitations of LLM Benchmarks: Overfitting and Training Data Leakage  </li> <li>Evaluating Large Language Models: 6 Next-Level Benchmarks Unveiled  </li> <li>HuggingFace OpenLLM Leaderboard: Comparing Open-Source Language Models  </li> <li>Master LLM Leaderboards: Comparing Open Source and Closed Source Models  </li> <li>Comparing LLMs: Top 6 Leaderboards for Evaluating Language Models  </li> <li>Specialized LLM Leaderboards: Finding the Best Model for Your Use Case  </li> <li>LLAMA vs GPT-4: Benchmarking Large Language Models for Code Generation  </li> <li>Human-Rated Language Models: Understanding the LM Sys Chatbot Arena  </li> <li>Commercial Applications of Large Language Models: From Law to Education  </li> <li>Comparing Frontier and Open-Source LLMs for Code Conversion Projects  </li> <li>Leveraging Frontier Models for High-Performance Code Generation in C++  </li> <li>Comparing Top LLMs for Code Generation: GPT-4 vs Claude 3.5 Sonnet  </li> <li>Optimizing Python Code with Large Language Models: GPT-4 vs Claude 3.5  </li> <li>Code Generation Pitfalls: When Large Language Models Produce Errors  </li> <li>Blazing Fast Code Generation: How Claude Outperforms Python by 13,000x  </li> <li>Building a Gradio UI for Code Generation with Large Language Models  </li> <li>Optimizing C++ Code Generation: Comparing GPT and Claude Performance  </li> <li>Comparing GPT-4 and Claude for Code Generation: Performance Benchmarks  </li> <li>Open Source LLMs for Code Generation: Hugging Face Endpoints Explored  </li> <li>How to Use HuggingFace Inference Endpoints for Code Generation Models  </li> <li>Integrating Open-Source Models with Frontier LLMs for Code Generation  </li> <li>Comparing Code Generation: GPT-4, Claude, and CodeQuen LLMs  </li> <li>Code Generation with LLMs: Techniques and Model Selection  </li> <li>Evaluating LLM Performance: Model-Centric vs Business-Centric Metrics  </li> <li>LLM Code Generation: Advanced Challenges for Python Developers</li> </ul>"},{"location":"#5-rag-with-langchain","title":"5: RAG with LangChain","text":"<p>This module shows how to augment LLMs with external knowledge through retrieval-augmented generation. You will learn the fundamentals of RAG-why and how to leverage external data to improve response accuracy-and build a DIY RAG system from scratch. You\u2019ll explore vector embeddings, set up OpenAI and Chroma stores, and use LangChain\u2019s text splitters and pipeline abstractions to assemble efficient retrieval workflows. Hands-on tutorials will cover embedding visualization with t-SNE, FAISS vs. Chroma comparisons, pipeline debugging, and troubleshooting common issues. By the end, you will have built your own AI knowledge-worker capable of combining LLM reasoning with up-to-date information.</p> Click to expand Module Overview <ul> <li>RAG Fundamentals: Leveraging External Data to Improve LLM Responses  </li> <li>Building a DIY RAG System: Implementing Retrieval-Augmented Generation  </li> <li>Understanding Vector Embeddings: The Key to RAG and LLM Retrieval  </li> <li>Unveiling LangChain: Simplify RAG Implementation for LLM Applications  </li> <li>LangChain Text Splitter Tutorial: Optimizing Chunks for RAG Systems  </li> <li>Preparing for Vector Databases: OpenAI Embeddings and Chroma in RAG  </li> <li>Vector Embeddings: OpenAI and Chroma for LLM Engineering  </li> <li>Visualizing Embeddings: Exploring Multi-Dimensional Space with t-SNE  </li> <li>Building RAG Pipelines: From Vectors to Embeddings with LangChain  </li> <li>Implementing RAG Pipeline: LLM, Retriever, and Memory in LangChain  </li> <li>Retrieval-Augmented Generation: Hands-On LLM Integration  </li> <li>Master RAG Pipeline: Building Efficient RAG Systems  </li> <li>Optimizing RAG Systems: Troubleshooting and Fixing Common Problems  </li> <li>Switching Vector Stores: FAISS vs Chroma in LangChain RAG Pipelines  </li> <li>Demystifying LangChain: Behind-the-Scenes of RAG Pipeline Construction  </li> <li>Debugging RAG: Optimizing Context Retrieval in LangChain  </li> <li>Build Your Personal AI Knowledge Worker: RAG for Productivity Boost</li> </ul>"},{"location":"#6-fine-tuning-with-loraqlora","title":"6: Fine-Tuning with LoRA/QLoRA","text":"<p>This module covers end-to-end fine-tuning of large language models using parameter-efficient methods. You will learn how to source and curate balanced datasets, apply scrubbing techniques, and engineer features for LLM training. You\u2019ll compare model- and business-centric evaluation metrics, build baseline ML models, and analyze price-description correlations. Hands-on tutorials will guide you through preparing JSONL files, launching OpenAI fine-tuning jobs, and tracking training progress with Weights &amp; Biases. Finally, you\u2019ll explore challenges in loss monitoring, hyperparameter optimization for LoRA/QLoRA, and best practices for productionizing fine-tuned models at scale.</p> Click to expand Module Overview <ul> <li>Fine-Tuning Large Language Models: From Inference to Training  </li> <li>Finding and Crafting Datasets for LLM Fine-Tuning: Sources &amp; Techniques  </li> <li>Data Curation Techniques for Fine-Tuning LLMs on Product Descriptions  </li> <li>Optimizing Training Data: Scrubbing Techniques for LLM Fine-Tuning  </li> <li>Evaluating LLM Performance: Model-Centric vs Business-Centric Metrics  </li> <li>LLM Deployment Pipeline: From Business Problem to Production Solution  </li> <li>Prompting, RAG, and Fine-Tuning: When to Use Each Approach  </li> <li>Productionizing LLMs: Best Practices for Deploying AI Models at Scale  </li> <li>Optimizing Large Datasets for Model Training: Data Curation Strategies  </li> <li>How to Create a Balanced Dataset for LLM Training: Curation Techniques  </li> <li>Finalizing Dataset Curation: Analyzing Price-Description Correlations  </li> <li>How to Create and Upload a High-Quality Dataset on HuggingFace  </li> <li>Feature Engineering and Bag of Words: Building ML Baselines for NLP  </li> <li>Baseline Models in ML: Implementing Simple Prediction Functions  </li> <li>Feature Engineering Techniques for Amazon Product Price Prediction  </li> <li>Optimizing LLM Performance: Advanced Feature Engineering Strategies  </li> <li>Linear Regression for LLM Fine-Tuning: Baseline Model Comparison  </li> <li>Bag of Words NLP: Implementing Count Vectorizer for Text Analysis in ML  </li> <li>Support Vector Regression vs Random Forest: Machine Learning Face-Off  </li> <li>Comparing Traditional ML Models: From Random to Random Forest  </li> <li>Evaluating Frontier Models: Comparing Performance to Baseline Frameworks  </li> <li>Human vs AI: Evaluating Price Prediction Performance in Frontier Models  </li> <li>GPT-4o Mini: Frontier AI Model Evaluation for Price Estimation Tasks  </li> <li>Comparing GPT-4 and Claude: Model Performance in Price Prediction Tasks  </li> <li>Frontier AI Capabilities: LLMs Outperforming Traditional ML Models  </li> <li>Fine-Tuning LLMs with OpenAI: Preparing Data, Training, and Evaluation  </li> <li>How to Prepare JSONL Files for Fine-Tuning Large Language Models (LLMs)  </li> <li>Step-by-Step Guide: Launching GPT Fine-Tuning Jobs with OpenAI API  </li> <li>Fine-Tuning LLMs: Track Training Loss &amp; Progress with Weights &amp; Biases  </li> <li>Evaluating Fine-Tuned LLMs Metrics: Analyzing Training &amp; Validation Loss  </li> <li>LLM Fine-Tuning Challenges: When Model Performance Doesn't Improve  </li> <li>Fine-Tuning Frontier LLMs: Challenges &amp; Best Practices for Optimization</li> </ul>"},{"location":"#7-fine-tuned-price-prediction-model","title":"7: Fine-Tuned Price Prediction Model","text":"<p>This module delivers a deep dive into building a high-performance price-prediction model through parameter-efficient fine-tuning techniques. You will master LoRA and QLoRA adaptors, learn advanced quantization methods (8-bit, NF4), and analyze their impact on model size and accuracy. Guided tutorials cover selecting and tokenizing the optimal base model, configuring SFTTrainer for 4-bit fine-tuning, and tuning epochs, batch sizes, learning rates, and optimizers. You will launch QLoRA training jobs, monitor loss and performance with Weights &amp; Biases, apply cost-saving strategies like smaller datasets, and visualize training metrics. Finally, you will evaluate your proprietary fine-tuned LLM against GPT-4 and business benchmarks, refining hyperparameters to achieve best-in-class price-prediction results.</p>"},{"location":"01_first_llm_product/01_jumping_right_llm/","title":"Jumping Right into LLM Engineering","text":"<p>This documents the launch of an eight-week LLM engineering program, beginning with an immediate, hands-on environment setup and progressing through model selection, fine-tuning, and deployment best practices. Key outcomes include accelerated learner engagement, streamlined technical workflows, and a foundation for scalable LLM applications.</p> <pre><code>graph TD;\n    A[Start: My LLM Journey] --&gt; B[Build Your First LLM Product];\n    B --&gt; C[Build a Multi-Modal Chatbot];\n    C --&gt; D[Open-Source Gen AI Solutions];\n    D --&gt; E[Model Evaluation for Code &amp; Business Tasks];\n    E --&gt; F[RAG with LangChain];\n    F --&gt; G[Fine-Tuning with LoRA/QLoRA];\n    G --&gt; H[Fine-Tuned Price Prediction Model];\n    H --&gt; I[Autonomous Multi-Agent Systems];\n    I --&gt; J[Mastery];</code></pre>"},{"location":"01_first_llm_product/01_jumping_right_llm/#context-and-objectives","title":"Context and Objectives","text":""},{"location":"01_first_llm_product/01_jumping_right_llm/#project-goals","title":"Project Goals","text":"<ul> <li>Equip participants to install, configure, and run an LLM locally.</li> <li>Develop practical skills in prompt design, model adaptation, and deployment.</li> </ul>"},{"location":"01_first_llm_product/01_jumping_right_llm/#scope","title":"Scope","text":"<ul> <li>Covering environment setup, core architecture, fine-tuning, evaluation, and deployment.</li> <li>Emphasis on open-source tools and reproducible pipelines.</li> </ul>"},{"location":"01_first_llm_product/01_jumping_right_llm/#appendices","title":"Appendices","text":""},{"location":"01_first_llm_product/01_jumping_right_llm/#glossary-of-terms","title":"Glossary of Terms","text":"<ul> <li>LLM: Large Language Model</li> <li>Tokenization: Converting raw text into discrete model input units</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/","title":"Setting Up Ollama for Local LLM Deployment","text":"<ul> <li>The project established an on-premise inference pipeline for Meta\u2019s open-source LLaMA 3 model using the Ollama platform.</li> <li>A prototype Korean tutoring application was implemented via prompt engineering, demonstrating cost-free, offline LLM utility.</li> <li>Cross-platform installation workflows and user-driven refinements validated both usability and performance benchmarks.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#context-and-objectives","title":"Context and Objectives","text":""},{"location":"01_first_llm_product/02_setting-up-ollama/#background","title":"Background","text":"<ul> <li>Rise of proprietary LLM API costs prompted investigation of local inference solutions.</li> <li>Meta\u2019s LLaMA series offers competitive open-source weights enabling self-hosted models.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#primary-objectives","title":"Primary Objectives","text":"<ul> <li>Deploy LLaMA 3 locally on both Windows and macOS with minimal dependencies.</li> <li>Validate interactive tutor use case for language instruction without external service fees.</li> <li>Document end-to-end workflow for developer adoption.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#scope-and-constraints","title":"Scope and Constraints","text":"<ul> <li>Platforms: Windows 10+ (PowerShell) and macOS (bash/zsh).</li> <li>Model: LLaMA 3, using the 8B variant. No official 2B model is available from Meta.</li> <li>Hardware: CPU inference only; GPU support deferred to future phases.</li> <li>Network: Initial download required; subsequent runs fully offline.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#technical-deep-dive","title":"Technical Deep Dive","text":""},{"location":"01_first_llm_product/02_setting-up-ollama/#architectural-overview","title":"Architectural Overview","text":"<ul> <li>Ollama CLI: C++ executable providing model management, download, and inference.</li> <li>Model Variant: <code>llama3:8b</code>, optimized for CPU performance.</li> <li> <p>Runtime Environment:</p> </li> <li> <p>Windows: PowerShell with manual <code>PATH</code> configuration to include Ollama binary, e.g.:</p> <pre><code>$env:Path += \";C:\\Program Files\\Ollama\"\n</code></pre> </li> <li> <p>macOS: Homebrew installation or direct binary invocation in terminal.</p> </li> <li> <p>Storage: Model weights cached under <code>~/.ollama/models/blobs</code>; reused across sessions via internal Ollama indexing.</p> </li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#dependency-management","title":"Dependency Management","text":"<ul> <li>Windows Prerequisites: No external dependencies; native binary installer (no .NET required).</li> <li>macOS Prerequisites: Standard Xcode CLI tools; optional Homebrew.</li> <li>Networking: HTTPS endpoint for weight download; retry logic built into Ollama downloader.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#inference-workflow","title":"Inference Workflow","text":"<pre><code># Windows PowerShell\nollama run llama3:8b\n\n# macOS Terminal\nollama run llama3:8b\n</code></pre> <ul> <li>On first invocation, progress bar displays byte-level download status.</li> <li>Subsequent invocations initiate immediate interactive REPL.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#prompt-engineering-strategy","title":"Prompt Engineering Strategy","text":"<ul> <li>Initial Prompt:</li> </ul> <pre><code>I am trying to learn Korean. \nI am a complete beginner. \nPlease chat with me in basic Korean to teach me.\n</code></pre> <ul> <li> <p>Iterative Refinements:</p> </li> <li> <p>Enforced Korean punctuation and structural formatting.</p> </li> <li>Specified response format: greeting, example phrase, correction hints.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#challenges-and-mitigations","title":"Challenges and Mitigations","text":"<ul> <li>Download Latency: Users reported variable network speeds; resolved by exposing download progress and allowing resume on failure.</li> <li>Cross-Platform Differences: Path and execution semantics documented in separate OS-specific READMEs.</li> <li>Prompt Sensitivity: Minor typographic errors led to misformatted output; mitigated via explicit punctuation rules in prompt.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#lessons-learned-and-best-practices","title":"Lessons Learned and Best Practices","text":"<ul> <li>Local Inference Viability: On-premise LLMs eliminate per-token charges and improve data privacy.</li> <li>Model Selection Tradeoffs: Smaller models offer faster CPU inference but may underperform on complex tasks.</li> <li>User-Centered Prompt Design: Clearly specify role, proficiency, and expected output format to reduce ambiguity.</li> <li>Documentation Granularity: Separate OS-specific sections prevent cross-platform confusion.</li> <li>Resumable Downloads: Built-in retry and progress reporting enhance user experience on unstable networks.</li> <li>Offline Capability: Caching of model weights ensures functionality without persistent connectivity.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#recommendations-and-next-steps","title":"Recommendations and Next Steps","text":"<ul> <li> <p>GPU Acceleration Enablement</p> </li> <li> <p>Integrate CUDA (Linux/Windows) and Metal (macOS) backends for multi-fold speedups.</p> </li> <li> <p>Benchmark CPU vs GPU performance to quantify gains.</p> </li> <li> <p>Fine-Tuning Pipeline Development</p> </li> <li> <p>Establish data ingestion and preprocessing workflows.</p> </li> <li>Implement LoRA or full-parameter fine-tuning on curated Korean tutoring dataset.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#user-interface-enhancement","title":"User Interface Enhancement","text":"<ul> <li>Build minimal web frontend (React) to manage sessions, store conversation history, and visualize progress metrics.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#automated-evaluation-framework","title":"Automated Evaluation Framework","text":"<ul> <li>Define evaluation metrics (BLEU, perplexity) for language tutoring quality.</li> <li>Implement batch testing harness for regression monitoring.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#scalability-and-load-testing","title":"Scalability and Load Testing","text":"<ul> <li>Simulate concurrent users on shared hardware.</li> <li>Instrument resource usage (CPU, memory, disk) and optimize binary flags.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#appendices","title":"Appendices","text":""},{"location":"01_first_llm_product/02_setting-up-ollama/#glossary","title":"Glossary","text":"<ul> <li>Ollama: Command-line platform for local LLM hosting.</li> <li>LLaMA 3: Meta\u2019s open-source LLM; this project uses the 8B variant.</li> <li>REPL: Read-Eval-Print Loop for interactive model sessions.</li> <li>LoRA: Low-Rank Adaptation, parameter-efficient fine-tuning method.</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#referenced-tools-and-frameworks","title":"Referenced Tools and Frameworks","text":"<ul> <li>PowerShell (Windows)</li> <li>bash/zsh (macOS)</li> <li>Homebrew (macOS package manager)</li> <li>CUDA Toolkit (future GPU support)</li> <li>Metal Performance Shaders (future macOS GPU support)</li> </ul>"},{"location":"01_first_llm_product/02_setting-up-ollama/#code-snippet-sample-prompt-with-format-enforcement","title":"Code Snippet: Sample Prompt with Format Enforcement","text":"<pre><code>ollama run llama3:latest \"\nYou are a Korean tutor.\nUser level: Beginner.\nRespond with:\n1. Greeting in Korean.\n2. One example phrase.\n3. Correction hints for user input.\n\"\n</code></pre>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/","title":"LLM Engineering Roadmap","text":"<ul> <li>This document provides a structured overview of an eight-week LLM development and deployment initiative.</li> <li>The program explored frontier and open-source models, implemented multimodal and code generation solutions, and culminated in a collaborative agentic AI system for commercial use.</li> <li>The outcome is a reproducible framework for full-cycle LLM engineering.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#context-and-objectives","title":"Context and Objectives","text":""},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#project-goals","title":"Project Goals","text":"<ul> <li>Empower participants to become proficient in LLM engineering.</li> <li>Explore both closed-source (e.g., GPT-4.5, Claude 3.5) and open-source LLMs.</li> <li>Develop multiple production-ready AI systems: chat assistants, code translators, RAG systems, and agentic workflows.</li> <li>Establish best practices in prompt engineering, model selection, multimodality, and fine-tuning.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#scope","title":"Scope","text":"<p>The journey covers:</p> <ul> <li>Model experimentation and UI integration.</li> <li>Full-stack deployment of AI assistants.</li> <li>Training and evaluation of open-source models.</li> <li>Design and execution of a flagship business-driven agentic AI project.</li> </ul> Topic Decision/Outcome Frontier Models Evaluated GPT-4.5 preview and Claude 3.5 using both UI and API access. Began designing a commercial AI solution. UI Integration with Gradio Developed a multimodal chatbot with image, audio, and tool-calling capabilities using Gradio. Hugging Face Open Source Utilized Hugging Face pipelines and advanced APIs. Introduced tokenizers and model management. Model Selection Strategy Analyzed model selection criteria using leaderboards and benchmarks. Initiated a Python-to-C++ code generation project. Retrieval-Augmented Generation (RAG) Built a RAG pipeline for organizational question answering. Applied it to personal data as a commercial challenge. Business Problem Setup Defined a business challenge and created traditional ML baselines. Began fine-tuning frontier models. Open Source Fine-Tuning Improved open-source model performance to compete with GPT-4 via fine-tuning. Agentic AI Deployment Built a 7-agent autonomous system for solving a real commercial task. Enabled web-scraping and push notifications."},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#technical-deep-dive","title":"Technical Deep Dive","text":""},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#model-selection-and-architecture","title":"Model Selection and Architecture","text":"<ul> <li>Frontier models: GPT-4.5 preview, Claude 3.5.</li> <li>Open-source models: Hugging Face Transformers via <code>pipelines</code> and custom <code>AutoModel</code> APIs.</li> <li>UI Framework: Gradio for multimodal interaction and tool use.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#data-pipelines","title":"Data Pipelines","text":"<ul> <li>RAG data ingestion pipelines for organizational knowledge bases.</li> <li>Tokenization and preprocessing pipelines using Hugging Face Tokenizers.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#fine-tuning-approach","title":"Fine-Tuning Approach","text":"<ul> <li>Frontier models via APIs for prompt-tuning and inference.</li> <li>Open-source models via supervised fine-tuning loops with comparative benchmarks.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#challenges-and-resolutions","title":"Challenges and Resolutions","text":"<ul> <li>Multimodal integration: Simplified using Gradio, which abstracted audio/image I/O.</li> <li>Model selection: Resolved using comparative analysis across benchmarks.</li> <li>Performance tuning: Achieved 60,000x speed-up in C++ code generation via model comparison and fine-tuning.</li> <li>Open-source performance gaps: Mitigated through iterative tuning until models approached frontier performance.</li> </ul>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#lessons-learned-and-best-practices","title":"Lessons Learned and Best Practices","text":"<ol> <li>Experiment Widely with Models: Different models excel at different tasks (e.g., explanation vs. language understanding).</li> <li>Multimodal UIs Enhance Value: Incorporating voice, vision, and code tools creates a more useful assistant.</li> <li>Use Benchmarks for Model Selection: Avoid subjective picking; rely on leaderboard performance and task-specific metrics.</li> <li>Code Translation Tasks Reveal Latent Power: Use code generation as a diagnostic for real-world utility.</li> <li>RAG Pipelines Improve Organizational Alignment: Custom data integration with retrieval elevates LLM contextuality.</li> </ol>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#recommendations-and-next-steps","title":"Recommendations and Next Steps","text":"<ol> <li>Productionize Agentic Frameworks: Expand the Week 8 multi-agent architecture into a commercial SaaS offering.</li> <li>Evaluate Long-Context Models: Test models like Claude 3.5 and GPT-4 Turbo for document-heavy use cases.</li> <li>Model Lifecycle Automation: Introduce CI/CD pipelines for training, validation, and deployment of LLM-backed services.</li> </ol>"},{"location":"01_first_llm_product/03_llm-engineering-roadmap/#glossary","title":"Glossary","text":"<ul> <li>RAG: Retrieval-Augmented Generation</li> <li>Gradio: Python UI framework for ML demos</li> <li>Hugging Face: Open-source platform for model distribution</li> <li>Agentic AI: Autonomous multi-agent system solving tasks collaboratively</li> <li>Fine-Tuning: Post-training adaptation on specific data</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/","title":"LLM API Integration for Engineers","text":"<p>This document serves as a comprehensive guide for Large Language Model (LLM) engineers focused on integrating frontier LLM APIs\u2014OpenAI (GPT-4), Anthropic (Claude), and Google (Gemini)\u2014into their engineering workflows. It expands upon foundational concepts and focuses on real-world application through API calls, multi-model interoperability, and environment setup best practices.</p>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before proceeding, the engineer should have the following skills and setup completed:</p>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#prerequisite-knowledge","title":"\u2705 Prerequisite Knowledge","text":"<ul> <li> <p>Understanding of:</p> </li> <li> <p>Tokenization and context windows.</p> </li> <li>Transformer architecture fundamentals.</li> <li>Prompt engineering basics (system/user prompts, one-shot prompting).</li> <li> <p>Practical experience using:</p> </li> <li> <p>Six major frontier LLMs via UI.</p> </li> <li>OpenAI API (GPT-4.0) for streaming, markdown, JSON output, chaining calls.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#environment","title":"\u2705 Environment","text":"<ul> <li><code>JupyterLab</code> installed and operational.</li> <li><code>.env</code> file configured for secret management and <code>git</code>-ignored.</li> <li>OpenAI API key configured.</li> <li>Python coding proficiency for API interaction.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#2-objectives","title":"2. Objectives","text":"<p>The goals for this week are:</p> <ol> <li>Integrate Anthropic Claude API</li> <li>Integrate Google Gemini API</li> <li>Implement Interoperable Code Between Models</li> <li>Prepare Environment for Multi-LLM Usage</li> </ol>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#3-claude-anthropic-api-integration","title":"3. Claude (Anthropic) API Integration","text":""},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#api-key-setup","title":"API Key Setup","text":"<ol> <li>Visit: https://www.anthropic.com</li> <li>Sign up and generate your API key.</li> <li>Copy the key to your project\u2019s <code>.env</code> file:</li> </ol> <pre><code>ANTHROPIC_API_KEY=your_key_here\n</code></pre>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#advantages","title":"Advantages","text":"<ul> <li>Easy registration.</li> <li>Free credits offered upon first-time sign-up (as of transcript date).</li> <li>Similar UX to OpenAI.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#sample-usage-python","title":"Sample Usage (Python)","text":"<pre><code>import os\nimport anthropic\n\nclient = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n\nresponse = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1000,\n    temperature=0.7,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain diffusion models in simple terms.\"}\n    ]\n)\n\nprint(response.content)\n</code></pre>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#4-gemini-google-api-integration","title":"4. Gemini (Google) API Integration","text":""},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#api-key-setup_1","title":"API Key Setup","text":"<ol> <li>Sign in to Google Cloud Console</li> <li>Enable the Generative Language API.</li> <li>Create a service account with appropriate roles.</li> <li>Generate an API key.</li> <li>Add to your <code>.env</code> file:</li> </ol> <pre><code>GOOGLE_API_KEY=your_key_here\n</code></pre>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#caveats","title":"Caveats","text":"<ul> <li>Complex and non-intuitive interface.</li> <li>May involve navigating multiple permission screens.</li> <li>Failure-prone for first-timers.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#workaround","title":"Workaround","text":"<ul> <li>Optional integration. All examples can be executed with just OpenAI and Anthropic if Gemini setup is too problematic.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#5-api-key-security","title":"5. API Key Security","text":""},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#recommended-practice","title":"Recommended Practice","text":"<ul> <li>Store keys in a <code>.env</code> file located at your project root.</li> <li>Ensure the <code>.env</code> file is added to <code>.gitignore</code>.</li> </ul>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#alternative-not-recommended","title":"Alternative (Not Recommended)","text":"<ul> <li>Hardcoding keys in Jupyter Notebook for quick testing:</li> </ul> <p><pre><code>API_KEY = \"your_key_here\"\n</code></pre> * Warning: Never push hardcoded secrets to version control.</p>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#6-multi-model-interaction-example","title":"6. Multi-Model Interaction Example","text":"<p>This code illustrates switching between APIs in a unified function:</p> <pre><code>def call_model(provider, prompt):\n    if provider == \"openai\":\n        import openai\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n        return openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        ).choices[0].message.content\n\n    elif provider == \"anthropic\":\n        import anthropic\n        client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n        return client.messages.create(\n            model=\"claude-3-opus-20240229\",\n            max_tokens=1000,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        ).content\n\n    elif provider == \"google\":\n        import google.generativeai as genai\n        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n        model = genai.GenerativeModel(\"gemini-pro\")\n        response = model.generate_content(prompt)\n        return response.text\n\n    else:\n        raise ValueError(\"Unsupported provider\")\n</code></pre>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#7-roadmap-preview","title":"7. Roadmap Preview","text":"Focus Area 1 Transformer basics, GPT-4 usage 2 Multi-API integration (Claude, Gemini) 3 UI Development, Agents, Multimodal 4 Hugging Face &amp; Open Source LLMs 5 Model Selection &amp; Code Generation 6 Retrieval-Augmented Generation (RAG) 7 Fine-tuning (Frontier &amp; OSS) 8 Capstone Projects &amp; Demo"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#8-next-steps","title":"8. Next Steps","text":"<ul> <li>Launch your development interface:</li> </ul> <p><pre><code>jupyter lab\n</code></pre> * Verify <code>.env</code> file includes all necessary keys. * Begin testing Claude and Gemini calls with aligned prompts. * Proceed to UI prototyping and agent integration next week.</p>"},{"location":"02_multi_modal_chatbot/01_multiple_ai_apis/#9-final-notes","title":"9. Final Notes","text":"<ul> <li>Focus on <code>.env</code> hygiene to avoid secret leakage.</li> <li>Claude integration is straightforward.</li> <li>Gemini setup is tedious but non-blocking.</li> <li>Multi-provider code sets you up for future model orchestration.</li> </ul>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/","title":"Streaming AI Responses","text":"<p>This document describes how to implement real-time LLM output (\u201cstreaming\u201d) in Python using four providers: OpenAI, Anthropic (Claude), Google (Gemini), and local models via Ollama. It covers environment setup, key management (where required), code examples for both synchronous and streaming interactions, API parameter explanations, and best practices.</p>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li> <p>For OpenAI, Anthropic, Google:</p> </li> <li> <p>Accounts and API keys</p> </li> <li> <p>Installed SDKs</p> <pre><code>pip install openai anthropic google-generativeai\n</code></pre> </li> <li> <p>For Ollama (local models):</p> </li> <li> <p>Docker (macOS or Linux) or Windows Subsystem for Linux (WSL)</p> </li> <li>Ollama daemon installed and running</li> <li> <p>Ollama Python client library</p> <pre><code>pip install ollama\n</code></pre> </li> </ul>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#environment-configuration","title":"Environment Configuration","text":"<ol> <li>Create a file named <code>.env</code> in your project root.</li> <li>Populate it with provider keys (skip for Ollama):</li> </ol> <pre><code>OPENAI_API_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\nGOOGLE_API_KEY=your_google_key\n</code></pre> <ol> <li>Load environment variables in code rather than embedding keys directly:</li> </ol> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"]   = os.getenv(\"OPENAI_API_KEY\")\nos.environ[\"ANTHROPIC_API_KEY\"]= os.getenv(\"ANTHROPIC_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"]   = os.getenv(\"GOOGLE_API_KEY\")\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#common-message-structure","title":"Common Message Structure","text":"<p>All four interfaces use a chat-style message list or a simple prompt. Each message is a dict with:</p> <ul> <li><code>role</code>: one of <code>\"system\"</code>, <code>\"user\"</code> (and <code>\"assistant\"</code> in responses).</li> <li><code>content</code>: the text to send.</li> </ul> <p>Additional parameters may include:</p> <ul> <li><code>model</code>: model identifier (e.g., <code>gpt-4</code>, <code>claude-3.5-sonnet</code>, <code>gemini-1.5-flash</code>, <code>llama3</code>)</li> <li><code>temperature</code>: float in [0,1] (higher = more creative)</li> <li><code>max_tokens</code> / <code>n_predict</code> / <code>num_predict</code>: cap on output length</li> <li><code>stream</code>: boolean for OpenAI; method switch or flag for others</li> </ul>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#connecting-to-apis","title":"Connecting to APIs","text":""},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#openai","title":"OpenAI","text":"<pre><code>import openai, os\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\":\"system\",\"content\":\"You are an assistant great at telling jokes.\"},\n        {\"role\":\"user\",\"content\":\"Tell a lighthearted joke for data scientists.\"}\n    ],\n    temperature=0.7\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#anthropic-claude","title":"Anthropic (Claude)","text":"<pre><code>from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\nimport os\n\nclient = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n\nresponse = client.chat.completions.create(\n    model=\"claude-3.5-sonnet\",\n    max_tokens_to_sample=256,\n    prompt=(HUMAN_PROMPT +\n            \"Tell a lighthearted joke for data scientists.\\n\" +\n            AI_PROMPT)\n)\nprint(response.completion)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#google-gemini","title":"Google (Gemini)","text":"<pre><code>import google.generativeai as genai\nimport os\n\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\ngemini = genai.models.TextGenerationModel.from_pretrained(\"gemini-1.5-flash\")\n\nresponse = gemini.generate(\n    prompt=\"Tell a lighthearted joke for data scientists.\",\n    temperature=0.7\n)\nprint(response.text)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#ollama-local-models","title":"Ollama (Local Models)","text":"<ol> <li>Pull or install a model, e.g.:</li> </ol> <pre><code>ollama pull llama3\n</code></pre> <ol> <li>Verify available models:</li> </ol> <pre><code>ollama list\n</code></pre> <ol> <li>Functional interface:</li> </ol> <pre><code>from ollama import chat\n\nresponse = chat(\n    model=\"llama3\",\n    messages=[{\"role\":\"user\",\"content\":\"Tell a lighthearted joke for data scientists.\"}],\n    options={\"temperature\":0.7,\"num_predict\":256}\n)\nprint(response[\"message\"][\"content\"])\n</code></pre> <ol> <li>Client interface:</li> </ol> <pre><code>from ollama import Client\n\nclient = Client()  # connects to localhost:11434\nresponse = client.chat(\n    model=\"llama3\",\n    messages=[{\"role\":\"user\",\"content\":\"Tell a lighthearted joke for data scientists.\"}],\n    options={\"temperature\":0.7,\"num_predict\":256}\n)\nprint(response.message.content)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#example-joke-generation","title":"Example: Joke Generation","text":"Provider Prompt Sample Output GPT-3.5-turbo \u201cTell a lighthearted joke for data scientists.\u201d \u201cWhy did the data scientists break up with their computer? It couldn\u2019t handle their complex relationship.\u201d GPT-4 same \u201cWhy did the data scientist break up with a statistician? Because she found him too mean.\u201d Claude 3.5 Sonnet same \u201cWhy do data scientists break up with their significant other? Too much variance in the relationship.\u201d Gemini 1.5 Flash same \u201cWhy did the data scientists break up with a statistician? Because they couldn\u2019t see eye to eye on the p value.\u201d Llama3 (Ollama) same \u201cWhy did the data scientist break up with their database? It lost all their relationships.\u201d <p>Adjust <code>temperature</code> or <code>num_predict</code> for variation in randomness and length.</p>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#streaming-responses","title":"Streaming Responses","text":""},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#openai-streaming","title":"OpenAI Streaming","text":"<pre><code>import sys\nfrom openai import ChatCompletion\n\nstream = ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=messages,\n    temperature=0.7,\n    stream=True\n)\nfor chunk in stream:\n    sys.stdout.write(chunk.choices[0].delta.get(\"content\",\"\"))\n    sys.stdout.flush()\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#anthropic-streaming","title":"Anthropic Streaming","text":"<pre><code>from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n\nclient = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\nwith client.chat.completions.stream(\n    model=\"claude-3.5-sonnet\",\n    max_tokens_to_sample=256,\n    prompt=HUMAN_PROMPT + user_prompt + AI_PROMPT\n) as stream:\n    for event in stream:\n        print(event.completion, end=\"\", flush=True)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#google-gemini_1","title":"Google Gemini","text":"<p>Streaming not supported; use synchronous only.</p>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#ollama-streaming","title":"Ollama Streaming","text":"<pre><code>from ollama import chat\n\nfor chunk in chat(\n    model=\"llama3\",\n    messages=[{\"role\":\"user\",\"content\":\"Explain how to decide if a business problem is suitable for an LLM solution.\"}],\n    stream=True,\n    options={\"temperature\":0.7,\"num_predict\":512}\n):\n    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n</code></pre> <p>Client-based equivalent:</p> <pre><code>from ollama import Client\n\nclient = Client()\nstream = client.chat(\n    model=\"llama3\",\n    messages=[{\"role\":\"user\",\"content\":\"Explain how to decide if a business problem is suitable for an LLM solution.\"}],\n    stream=True,\n    options={\"temperature\":0.7,\"num_predict\":512}\n)\nfor chunk in stream:\n    print(chunk.message.content, end=\"\", flush=True)\n</code></pre>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#handling-markdown-streaming-in-jupyter","title":"Handling Markdown Streaming in Jupyter","text":"<pre><code>from IPython.display import clear_output, Markdown, display\n\nbuffer = \"\"\nstream = openai.ChatCompletion.create(..., stream=True)\nfor chunk in stream:\n    buffer += chunk.choices[0].delta.get(\"content\",\"\")\n    clear_output(wait=True)\n    display(Markdown(buffer))\n</code></pre> <p>Use the same pattern for Anthropic and Ollama streams.</p>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#comparison-of-apis","title":"Comparison of APIs","text":"Feature OpenAI Anthropic (Claude) Google (Gemini) Ollama (Local) Endpoint style <code>ChatCompletion.create</code> <code>chat.completions.create</code> <code>model.generate</code> <code>chat(...)</code> / <code>Client.chat</code> Streaming activation <code>stream=True</code> <code>.stream(...)</code> method Not supported <code>stream=True</code> flag System message placement In messages list Separate <code>prompt</code> prefix In constructor In messages list Token cap param <code>max_tokens</code> <code>max_tokens_to_sample</code> Implicit <code>num_predict</code> Context window 8K\u2013128K tokens \\~100K tokens 1 000 000 tokens Model-dependent"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#best-practices","title":"Best Practices","text":"<ul> <li>Key Management: Store in <code>.env</code>, never hard-code (not required for Ollama).</li> <li>Error Handling: Catch provider-specific exceptions.</li> <li>Rate Limits: Respect external API limits; local models unrestricted.</li> <li>Chunk Processing: Accumulate partial content; avoid per-chunk newlines.</li> <li>Token Budgeting: Use <code>max_tokens</code>/<code>num_predict</code> to control length.</li> <li>Reproducibility: Set <code>temperature=0</code> for deterministic output.</li> <li>Local vs Cloud: Use Ollama for offline or privacy-sensitive tasks.</li> </ul>"},{"location":"02_multi_modal_chatbot/02_streaming_ai_responses/#next-steps","title":"Next Steps","text":"<ul> <li>Enable model-to-model \u201cself-play\u201d conversations.</li> <li>Integrate multi-turn context management.</li> <li>Explore fine-tuning or embeddings pipelines.</li> </ul>"},{"location":"03_open-source-solutions/overview/","title":"Overview","text":""},{"location":"04_model-evaluation/overview/","title":"Overview","text":""},{"location":"05_rag-with-langchain/overview/","title":"Overview","text":""},{"location":"06_fine-tuning/01_fine-tuning/","title":"Fine-Tuning Large Language Models: From Inference to Training","text":"<p>This documentation is designed for beginners to understand the concepts of fine-tuning large language models. It covers the transition from inference to training, the importance of data preparation, and introduces a practical example of fine-tuning a model for a specific task.</p>"},{"location":"06_fine-tuning/01_fine-tuning/#introduction","title":"Introduction","text":"<p>Fine-tuning large language models is a critical step in making these models more effective for specific tasks. This guide introduces the concepts of inference, training, and fine-tuning, with a focus on preparing and using data effectively. It also outlines a practical example of fine-tuning a model to estimate product prices based on their descriptions, which serves as an accessible and measurable problem for beginners.</p> <p>The following diagram illustrates the high-level process of moving from a pre-trained model to a fine-tuned model:</p> <pre><code>graph TD\n    A[Pre-trained LLM&lt;br&gt;General Knowledge] --&gt; B[Fine-Tuning Process&lt;br&gt;Task-Specific Data]\n    B --&gt; C[Fine-Tuned LLM&lt;br&gt;Specialized for Task]\n    C --&gt; D[Inference&lt;br&gt;Predicting Outputs]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#what-is-inference","title":"What is Inference?","text":"<p>Inference is the process of using a pre-trained language model to make predictions or generate outputs based on input data. For example, when you provide a sentence to a model and it predicts the next word or generates a response, that\u2019s inference.</p> <p>Key points about inference:</p> <ul> <li>Pre-trained Models: These models have been trained on vast amounts of data to understand language patterns.</li> <li>Runtime Predictions: During inference, the model uses its learned knowledge to predict the next token (e.g., a word or part of a word) based on the input it receives.</li> <li>Techniques to Improve Inference:</li> <li>Multi-shot Prompting: Providing multiple examples in the prompt to guide the model\u2019s output.</li> <li>Prompt Chaining: Sending multiple prompts in sequence, building on previous outputs to refine results.</li> <li>Tool Usage: Allowing the model to interact with external tools (e.g., calculating prices).</li> <li>Retrieval-Augmented Generation (RAG): Injecting relevant context into the prompt to improve the model\u2019s output.</li> </ul> <p>Inference focuses on optimizing how we use an existing model without changing its internal structure. The following diagram shows the inference process:</p> <pre><code>graph LR\n    A[Input Prompt] --&gt; B[Pre-trained LLM]\n    B --&gt; C[Output Prediction&lt;br&gt; e.g., Next Token, Text, JSON]\n    C --&gt; D[Refined Output&lt;br&gt; Using Prompting Techniques]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#transitioning-to-training","title":"Transitioning to Training","text":"<p>Training involves modifying the internal parameters (or weights) of a neural network to improve its performance for a specific task. Unlike inference, which uses a pre-trained model as-is, training adjusts the model to better understand a particular problem or dataset.</p> <p>Key differences:</p> <ul> <li>Inference: Uses a fixed model to generate outputs.</li> <li>Training: Updates the model\u2019s parameters to improve its predictions.</li> </ul> <p>Training a large language model from scratch is expensive, often costing millions of dollars due to the computational resources required for models with billions of parameters. However, transfer learning makes training more accessible by allowing us to start with a pre-trained model and fine-tune it for a specific task.</p> <p>The diagram below illustrates the transition from inference to training:</p> <pre><code>graph TD\n    A[Inference&lt;br&gt; Using Pre-trained Model ] --&gt; B[Training&lt;br&gt; Adjusting Model Parameters]\n    B --&gt; C[Fine-Tuning&lt;br&gt;Task-Specific Adjustments]\n    C --&gt; D[Improved Model&lt;br&gt;Better Task Performance]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#the-importance-of-data","title":"The Importance of Data","text":"<p>Data is the foundation of any successful machine learning project, including fine-tuning LLMs. While data preparation may not seem exciting, it is one of the most critical steps in the process.</p> <p>Key aspects of data preparation:</p> <ul> <li>Crafting a Dataset: Collecting and organizing data relevant to the task.</li> <li>Cleaning and Curating: Removing errors, duplicates, or irrelevant information to ensure high-quality data.</li> <li>Visualization: Exploring the data to understand its structure, patterns, and potential issues.</li> <li>Defining Success Metrics: Establishing clear, measurable goals to evaluate the model\u2019s performance.</li> </ul> <p>For example, if you\u2019re fine-tuning a model to estimate product prices, you need a dataset with product descriptions and their corresponding prices. The quality and relevance of this data directly impact the model\u2019s ability to make accurate predictions.</p> <p>The following diagram outlines the data preparation process:</p> <pre><code>graph TD\n    A[Raw Data&lt;br&gt;Product Descriptions, Prices] --&gt; B[Cleaning&lt;br&gt;Remove Errors, Duplicates]\n    B --&gt; C[Curating&lt;br&gt;Select Relevant Data]\n    C --&gt; D[Visualization&lt;br&gt;Understand Patterns]\n    D --&gt; E[Prepared Dataset&lt;br&gt;Ready for Fine-Tuning]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#what-is-fine-tuning","title":"What is Fine-Tuning?","text":"<p>Fine-tuning is the process of taking a pre-trained language model and continuing its training on a smaller, task-specific dataset. This allows the model to adapt its general knowledge to a specialized problem.</p> <p>Key points:</p> <ul> <li>Transfer Learning: Fine-tuning leverages the knowledge a model has already gained from large-scale training (e.g., understanding grammar, context, or general facts) and builds on it with task-specific data.</li> <li>Efficiency: Fine-tuning is less resource-intensive than training a model from scratch, making it feasible for smaller budgets.</li> <li>Techniques: Methods like QLoRA (Quantized Low-Rank Adaptation) are used to fine-tune models efficiently, reducing memory and computational requirements.</li> <li>Goal: Adjust the model\u2019s parameters to improve its performance on a specific task, such as generating more accurate outputs or solving a specialized problem.</li> </ul> <p>For example, a pre-trained model might understand language well but struggle to estimate the price of a washing machine based on its description. Fine-tuning with a dataset of product descriptions and prices helps the model learn this specific task.</p> <p>The diagram below shows the fine-tuning process:</p> <pre><code>graph TD\n    A[Pre-trained LLM&lt;br&gt;Billions of Parameters] --&gt; B[Task-Specific Dataset&lt;br&gt;e.g., Product Descriptions]\n    B --&gt; C[Fine-Tuning&lt;br&gt;Using QLoRA or Similar]\n    C --&gt; D[Fine-Tuned LLM&lt;br&gt;Specialized for Price Estimation]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#the-commercial-problem-price-estimation","title":"The Commercial Problem: Price Estimation","text":"<p>The example task in this guide is to fine-tune a model for an e-commerce company to estimate the price of products (e.g., electronics, appliances, or car accessories) based on their textual descriptions. This is a practical problem with a clear, measurable outcome.</p> <p>Key aspects:</p> <ul> <li>Input: A textual description of a product (e.g., \"A 55-inch 4K LED Smart TV with HDR\").</li> <li>Output: A numerical price estimate (e.g., $599.99).</li> <li>Dataset: A collection of product descriptions paired with their actual prices.</li> <li>Goal: Train the model to predict accurate prices based on descriptions alone.</li> </ul> <p>The diagram below illustrates the price estimation task:</p> <pre><code>graph LR\n    A[Product Description&lt;br&gt;Text Input] --&gt; B[Fine-Tuned LLM]\n    B --&gt; C[Price Estimate&lt;br&gt;Numerical Output]\n    C --&gt; D[Compare with Actual Price&lt;br&gt;Evaluate Accuracy]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#why-this-problem","title":"Why This Problem?","text":"<p>The price estimation task is chosen for several reasons:</p> <ul> <li>Clear Measurability: Success is easy to evaluate by comparing the model\u2019s predicted price to the actual price, unlike text generation tasks (e.g., translation or summarization), which require subjective or complex metrics.</li> <li>Suitability for LLMs: While price estimation is traditionally a regression problem, modern LLMs can handle it effectively by generating numerical outputs (e.g., in JSON format). Their emergent intelligence makes them surprisingly good at such tasks.</li> <li>Engaging and Practical: The problem is relatable and has real-world applications in e-commerce, making it an engaging challenge for beginners.</li> </ul> <p>The following diagram compares traditional regression models and LLMs for price estimation:</p> <pre><code>graph TD\n    A[Price Estimation Problem] --&gt; B[Traditional Regression Model&lt;br&gt;Predicts Numbers]\n    A --&gt; C[Fine-Tuned LLM&lt;br&gt;Generates Numbers in JSON]\n    B --&gt; D[Simple but Limited]\n    C --&gt; E[Flexible and Powerful]</code></pre>"},{"location":"06_fine-tuning/01_fine-tuning/#next-steps","title":"Next Steps","text":"<p>The next phase involves diving into the data preparation process, including collecting, cleaning, and visualizing the dataset. You\u2019ll also define success metrics to evaluate the fine-tuned model\u2019s performance. Subsequent steps will cover implementing the fine-tuning process and testing the model on the price estimation task.</p> <p>The diagram below outlines the overall workflow for the project:</p> <pre><code>graph TD\n    A[Collect Data&lt;br&gt;Product Descriptions, Prices] --&gt; B[Prepare Data&lt;br&gt;Clean, Curate, Visualize]\n    B --&gt; C[Fine-Tune LLM&lt;br&gt;Using Task-Specific Data]\n    C --&gt; D[Test Model&lt;br&gt;Price Estimation]\n    D --&gt; E[Evaluate Results&lt;br&gt;Compare Predicted vs. Actual Prices]\n    E --&gt; F[Iterate and Improve&lt;br&gt;Adjust Data or Fine-Tuning]</code></pre>"},{"location":"06_fine-tuning/overview/","title":"Overview","text":""},{"location":"07_fine-tuned-price-prediction/overview/","title":"Overview","text":""},{"location":"08_autonomous-multi-agent/overview/","title":"Overview","text":""}]}